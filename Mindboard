import cv2
import numpy as np
import time
import mediapipe as mp
import pyvirtualcam
from pyvirtualcam import PixelFormat
import pyttsx3
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import re
import keyboard
import string
import itertools
from collections import deque
import os # <-- New import
import google.generativeai as genai # <-- New import

# --- Gemini API Setup ---
try:
    # Attempt to configure Gemini with an API key from environment variables
    genai.configure(api_key="AIzaSyAWDLuWkl5w0uvPywqNROHeI1rjRcoIVjQ")    # Initialize the Gemini model
    gemini_model = genai.GenerativeModel('gemini-1.5-flash')
    print("✅ Gemini API configured successfully.")
except (KeyError, TypeError):
    print("⚠️ Gemini API key not found or invalid. Falling back to local GPT-2 model only.")
    gemini_model = None

# --- Eye Gaze Controlled Keyboard Application Constants and Setup ---
FRAME_WIDTH, FRAME_HEIGHT = 640, 480
ZONES_X, ZONES_Y = 3, 3
ZONE_WIDTH, ZONE_HEIGHT = FRAME_WIDTH // ZONES_X, FRAME_HEIGHT // ZONES_Y
DEAD_ZONE_RADIUS = 30
DWELL_TIME_SECONDS = 1.5
COOLDOWN_SECONDS = 2.0
BLINK_COOLDOWN_SECONDS = 0.5
PREDICTION_GRID_POSITIONS = [(2, 0), (2, 1), (2, 2)]

# MediaPipe setup for Eye Gaze
mirror_mode = False  # Initialize mirroring state
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)
NOSE_TIP_INDEX = 1
LEFT_EYE = [362, 385, 387, 263, 373, 380]
RIGHT_EYE = [33, 160, 158, 133, 153, 144]
EAR_THRESHOLD = 0.25
CONSEC_FRAMES = 3

# TTS for Eye Gaze
engine = pyttsx3.init()
engine.setProperty('rate', 150)

# --- Fallback Model: GPT-2 for Eye Gaze ---
# Load the local model and tokenizer
fallback_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
fallback_model = GPT2LMHeadModel.from_pretrained("gpt2")
fallback_model.eval()

# Grid and letters for Eye Gaze (No changes here)
letters = list(string.ascii_uppercase)
letter_groups = [letters[i:i + 5] for i in range(0, 25, 5)]
letter_groups.append(letters[25:])
main_zones = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 2)]
main_grid_group_map = dict(zip(main_zones, letter_groups))
subgrid_positions = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 2)]
subgrid_layout_by_zone = {}
for zone, group_letters in main_grid_group_map.items():
    layout = {}
    for i, letter in enumerate(group_letters):
        layout[subgrid_positions[i]] = letter
    layout[(2, 0)] = "BACK"
    layout[(2, 1)] = "SPACE"
    layout[(2, 2)] = "SPEAK"
    subgrid_layout_by_zone[zone] = layout


# --- Air Painting Application Constants and Setup --- (No changes here)
bpoints = [deque(maxlen=1024)]
gpoints = [deque(maxlen=1024)]
rpoints = [deque(maxlen=1024)]
ypoints = [deque(maxlen=1024)]
blue_index = 0
green_index = 0
red_index = 0
yellow_index = 0
kernel = np.ones((5, 5), np.uint8)
colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (0, 255, 255)]
colorIndex = 0
BUTTON_BAR_HEIGHT = 65

# initialize mediapipe for Air Painting
mpHands = mp.solutions.hands
hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)
mpDraw = mp.solutions.drawing_utils


# --- Common Functions ---
def speak_text(text):
    engine.say(text)
    engine.runAndWait()

# --- MODIFIED: Prediction Function with Fallback ---
def predict_top3_words(prompt):
    """
    Predicts the top 3 most likely word completions or sentence continuations
    based on the user's input. Uses Gemini API if available, otherwise falls
    back to the local GPT-2 model.
    """
    prompt = prompt.strip()
    if not prompt:
        return ["I", "The", "You"]  # Default starting words

    # --- Primary Method: Try Gemini API ---
    if gemini_model:
        try:
            # Create a prompt for Gemini to focus on word and sentence completion
            gemini_prompt = (
                f"The user is trying to complete a sentence or word. "
                f"Given the input: '{prompt}', suggest the top 3 most likely next words "
                f"or word completions. Respond with only the three words, separated by a space."
            )
            response = gemini_model.generate_content(gemini_prompt)

            # Clean up the response from Gemini
            predicted_text = response.text.strip()
            # Extract words from the response
            words = re.findall(r'\b\w+\b', predicted_text)

            if words:
                print(f"✅ Gemini Prediction: {words[:3]}")
                return words[:3]
        except Exception as e:
            print(f"⚠️ Gemini API failed: {e}. Falling back to local model.")
            # Fall through to the local model if Gemini fails

    # --- Fallback Method: Local GPT-2 Model ---
    print("-> Using fallback GPT-2 model for prediction.")
    input_ids = fallback_tokenizer.encode(prompt, return_tensors='pt')
    with torch.no_grad():
        outputs = fallback_model.generate(
            input_ids,
            max_length=input_ids.shape[1] + 5,
            num_return_sequences=1,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            pad_token_id=fallback_tokenizer.eos_token_id
        )
    result_text = fallback_tokenizer.decode(outputs[0], skip_special_tokens=True)
    continuation = result_text[len(prompt):].strip()
    next_words = continuation.split()

    return next_words[:3] if next_words else []

def get_zone_from_nose(nose_x, nose_y):
    col = int(nose_x * ZONES_X)
    row = int(nose_y * ZONES_Y)
    return (row, col)

def calculate_EAR(eye_landmarks):
    def euclidean(p1, p2): return np.linalg.norm(np.array(p1) - np.array(p2))
    hor = euclidean(eye_landmarks[0], eye_landmarks[3])
    ver1 = euclidean(eye_landmarks[1], eye_landmarks[5])
    ver2 = euclidean(eye_landmarks[2], eye_landmarks[4])
    return (ver1 + ver2) / (2.0 * hor)

def highlight_current_zone(frame, zone, color=(0, 255, 255), thickness=4):
    if zone:
        r, c = zone
        x0, y0 = c * ZONE_WIDTH, r * ZONE_HEIGHT
        x1, y1 = x0 + ZONE_WIDTH, y0 + ZONE_HEIGHT
        cv2.rectangle(frame, (x0, y0), (x1, y1), color, thickness)

# --- Eye Gaze Application Function --- (No other changes needed in this function)
def run_eye_gaze_keyboard(cap, cam):
    global dwell_tracker, sentence, typed_characters, main_grid_active, selected_zone, current_subgrid, last_selection_time, blink_counter, blink_total, mode, auto_zone_cycle, auto_current_zone, last_auto_time, last_blink_time, prediction_words
    global mirror_output  # Access the global mirroring state

    # States reset
    dwell_tracker = {"last_zone": None, "start_time": None, "confirmed": False}
    sentence = ""
    typed_characters = []
    main_grid_active = True
    selected_zone = None
    current_subgrid = {}
    last_selection_time = 0
    blink_counter = 0
    blink_total = 0
    mode = "manual"
    auto_zone_cycle = itertools.cycle([(i, j) for i in range(3) for j in range(3)])
    auto_current_zone = next(auto_zone_cycle)
    last_auto_time = time.time()
    last_blink_time = 0
    prediction_words = []

    def update_dwell_tracker(current_zone, active):
        if not active or current_zone is None:
            dwell_tracker.update({"last_zone": None, "start_time": None, "confirmed": False})
            return None
        if dwell_tracker["last_zone"] != current_zone:
            dwell_tracker["last_zone"] = current_zone
            dwell_tracker["start_time"] = time.time()
            dwell_tracker["confirmed"] = False
            return None
        if time.time() - dwell_tracker["start_time"] >= DWELL_TIME_SECONDS and not dwell_tracker["confirmed"]:
            dwell_tracker["confirmed"] = True
            return current_zone
        return None

    def detect_blink(landmarks, w, h):
        global blink_counter, blink_total
        def coords(indices): return [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in indices]
        left_eye, right_eye = coords(LEFT_EYE), coords(RIGHT_EYE)
        left_EAR, right_EAR = calculate_EAR(left_eye), calculate_EAR(right_eye)
        avg_EAR = (left_EAR + right_EAR) / 2.0
        blink = False
        if avg_EAR < EAR_THRESHOLD:
            blink_counter += 1
        else:
            if blink_counter >= CONSEC_FRAMES:
                blink_total += 1
                blink = True
            blink_counter = 0
        return blink, avg_EAR

    while True:
        ret, frame = cap.read()
        if not ret:
            continue

        frame = cv2.flip(frame, 1)
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb)
        if mirror_mode:
            frame = cv2.flip(frame, 1)  # Flip the frame horizontally
        current_zone = None
        if results.multi_face_landmarks:
            face = results.multi_face_landmarks[0]
            nose = face.landmark[NOSE_TIP_INDEX]
            nose_px, nose_py = int(nose.x * FRAME_WIDTH), int(nose.y * FRAME_HEIGHT)
            if np.linalg.norm([nose_px - FRAME_WIDTH//2, nose_py - FRAME_HEIGHT//2]) > DEAD_ZONE_RADIUS:
                current_zone = get_zone_from_nose(nose.x, nose.y)
            blink, ear = detect_blink(face.landmark, FRAME_WIDTH, FRAME_HEIGHT)

        confirmed = update_dwell_tracker(current_zone, current_zone is not None)

        if keyboard.is_pressed('tab'):
            mode = "auto" if mode == "manual" else "manual"
            print(f"Mode switched to: {mode}")
            time.sleep(0.3)

        if keyboard.is_pressed('d'):
            print("Switching to Air Painting application.")
            return # Exit function to switch app

        if mode == "manual":
            highlight_current_zone(frame, current_zone)
        else:
            if time.time() - last_auto_time >= 1.5:
                auto_current_zone = next(auto_zone_cycle)
                last_auto_time = time.time()
            highlight_current_zone(frame, auto_current_zone, (255, 0, 255))
            if blink and (time.time() - last_blink_time > BLINK_COOLDOWN_SECONDS) and (time.time() - last_selection_time > COOLDOWN_SECONDS):
                last_blink_time = time.time()
                confirmed = auto_current_zone

        if confirmed and time.time() - last_selection_time > COOLDOWN_SECONDS:
            if main_grid_active and confirmed in main_grid_group_map:
                selected_zone = confirmed
                current_subgrid = subgrid_layout_by_zone[selected_zone]
                main_grid_active = False
            elif not main_grid_active and confirmed in current_subgrid:
                item = current_subgrid[confirmed]
                if item == "BACK":
                    main_grid_active = True
                elif item == "SPACE":
                    sentence += " "
                elif item == "SPEAK":
                    speak_text(sentence)
                    sentence = ""
                else:
                    sentence += item
                # Call prediction after a space or a word
                if sentence.endswith(" "):
                    prediction_words = predict_top3_words(sentence)
                else:
                    prediction_words = [] # Clear predictions while typing a word
            elif main_grid_active and confirmed in PREDICTION_GRID_POSITIONS and prediction_words:
                idx = PREDICTION_GRID_POSITIONS.index(confirmed)
                if idx < len(prediction_words):
                    # Add space before the predicted word if the sentence doesn't end with one
                    if sentence and not sentence.endswith(' '):
                        sentence += ' '
                    sentence += prediction_words[idx] + " " # Add a space after for next prediction
                    prediction_words = predict_top3_words(sentence) # Get new predictions
            
            last_selection_time = time.time()
            dwell_tracker = {"last_zone": None, "start_time": None, "confirmed": False}

        # --- UI Drawing ---
        for i in range(1, ZONES_X): cv2.line(frame, (i * ZONE_WIDTH, 0), (i * ZONE_WIDTH, FRAME_HEIGHT), (200, 200, 200), 1)
        for j in range(1, ZONES_Y): cv2.line(frame, (0, j * ZONE_HEIGHT), (FRAME_WIDTH, j * ZONE_HEIGHT), (200, 200, 200), 1)

        if main_grid_active:
            for zone, letters_group in main_grid_group_map.items():
                label = f"[{letters_group[0]}-{letters_group[-1]}]"
                cv2.putText(frame, label, (zone[1] * ZONE_WIDTH + 10, zone[0] * ZONE_HEIGHT + 40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            if prediction_words:
                for i, word in enumerate(prediction_words):
                    r, c = PREDICTION_GRID_POSITIONS[i]
                    cv2.putText(frame, word, (c * ZONE_WIDTH + 20, r * ZONE_HEIGHT + 60), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 200, 255), 2)
        else:
            for (r, c), item in current_subgrid.items():
                cv2.putText(frame, item, (c * ZONE_WIDTH + 20, r * ZONE_HEIGHT + 60), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 0), 3)

        cv2.putText(frame, "Sentence: " + sentence[-50:], (20, FRAME_HEIGHT - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        cv2.putText(frame, f"MODE: {mode.upper()} (Press TAB)", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        cv2.putText(frame, "Press 'D' to switch to Air Painting", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 1)

        # Flip the frame if mirroring is enabled
        frame_for_virtual_cam = cv2.flip(frame, 1) if mirror_output else frame
        cam.send(frame_for_virtual_cam)
        cam.sleep_until_next_frame()


# --- Air Painting Application Function --- (No changes here)
def run_air_painting(cap, cam):
    global bpoints, gpoints, rpoints, ypoints, blue_index, green_index, red_index, yellow_index, colorIndex

    # Reset painting state
    bpoints = [deque(maxlen=1024)]
    gpoints = [deque(maxlen=1024)]
    rpoints = [deque(maxlen=1024)]
    ypoints = [deque(maxlen=1024)]
    blue_index, green_index, red_index, yellow_index = 0, 0, 0, 0
    colorIndex = 0

    # Create a persistent canvas to draw on. This will be blended with the frame.
    paint_canvas = np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.uint8)

    def draw_buttons(frame):
        frame = cv2.rectangle(frame, (40, 1), (140, BUTTON_BAR_HEIGHT), (122, 122, 122), -1) # Clear
        frame = cv2.rectangle(frame, (160, 1), (255, BUTTON_BAR_HEIGHT), colors[0], -1) # Blue
        frame = cv2.rectangle(frame, (275, 1), (370, BUTTON_BAR_HEIGHT), colors[1], -1) # Green
        frame = cv2.rectangle(frame, (390, 1), (485, BUTTON_BAR_HEIGHT), colors[2], -1) # Red
        frame = cv2.rectangle(frame, (505, 1), (600, BUTTON_BAR_HEIGHT), colors[3], -1) # Yellow

        cv2.putText(frame, "CLEAR", (60, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        cv2.putText(frame, "BLUE", (185, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        cv2.putText(frame, "GREEN", (298, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        cv2.putText(frame, "RED", (420, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        cv2.putText(frame, "YELLOW", (520, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 2)
        return frame

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame = cv2.flip(frame, 1)
        framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Get hand landmark prediction
        result = hands.process(framergb)

        if result.multi_hand_landmarks:
            landmarks = []
            for handslms in result.multi_hand_landmarks:
                for lm in handslms.landmark:
                    lmx, lmy = int(lm.x * FRAME_WIDTH), int(lm.y * FRAME_HEIGHT)
                    landmarks.append([lmx, lmy])
                mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)
            
            fore_finger = (landmarks[8][0], landmarks[8][1])
            thumb = (landmarks[4][0], landmarks[4][1])
            cv2.circle(frame, fore_finger, 8, colors[colorIndex], -1)
            
            if abs(thumb[1] - fore_finger[1]) < 25 and abs(thumb[0] - fore_finger[0]) < 25: # Pinch gesture
                # A pinch gesture should not draw, so we reset the deques to create a new line segment later
                bpoints.append(deque(maxlen=512)); blue_index += 1
                gpoints.append(deque(maxlen=512)); green_index += 1
                rpoints.append(deque(maxlen=512)); red_index += 1
                ypoints.append(deque(maxlen=512)); yellow_index += 1
            
            elif fore_finger[1] <= BUTTON_BAR_HEIGHT: # Check if finger is in button area
                if 40 <= fore_finger[0] <= 140: # Clear
                    bpoints = [deque(maxlen=512)]; gpoints = [deque(maxlen=512)]; rpoints = [deque(maxlen=512)]; ypoints = [deque(maxlen=512)]
                    blue_index, green_index, red_index, yellow_index = 0,0,0,0
                    paint_canvas = np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), np.uint8)
                elif 160 <= fore_finger[0] <= 255: colorIndex = 0
                elif 275 <= fore_finger[0] <= 370: colorIndex = 1
                elif 390 <= fore_finger[0] <= 485: colorIndex = 2
                elif 505 <= fore_finger[0] <= 600: colorIndex = 3
            else: # Draw
                if colorIndex == 0: bpoints[blue_index].appendleft(fore_finger)
                elif colorIndex == 1: gpoints[green_index].appendleft(fore_finger)
                elif colorIndex == 2: rpoints[red_index].appendleft(fore_finger)
                elif colorIndex == 3: ypoints[yellow_index].appendleft(fore_finger)
        else: # No hand detected, create new line segment for next time
            bpoints.append(deque(maxlen=512)); blue_index += 1
            gpoints.append(deque(maxlen=512)); green_index += 1
            rpoints.append(deque(maxlen=512)); red_index += 1
            ypoints.append(deque(maxlen=512)); yellow_index += 1

        # Draw lines from the points arrays onto the persistent canvas
        points = [bpoints, gpoints, rpoints, ypoints]
        for i in range(len(points)):
            for j in range(len(points[i])):
                for k in range(1, len(points[i][j])):
                    if points[i][j][k - 1] is None or points[i][j][k] is None: continue
                    cv2.line(paint_canvas, points[i][j][k - 1], points[i][j][k], colors[i], 2)
        
        # Add the drawing canvas on top of the camera frame
        frame = cv2.add(frame, paint_canvas)

        # Draw UI on top of the combined frame+canvas
        frame = draw_buttons(frame)
        cv2.putText(frame, "Press 'D' to switch to Eye Gaze Keyboard", (20, FRAME_HEIGHT - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 1)

        # Send the final frame to the virtual camera
        cam.send(frame)
        cam.sleep_until_next_frame()

        if keyboard.is_pressed('d'):
            print("Switching to Eye Gaze Keyboard application.")
            return # Exit function to switch

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

if __name__ == "__main__":
    current_app = "eye_gaze"
    mirror_output = False  # Initialize mirroring state
    
    # Initialize camera and virtual camera ONCE.
    cap = cv2.VideoCapture(0)
    cap.set(3, FRAME_WIDTH)
    cap.set(4, FRAME_HEIGHT)

    with pyvirtualcam.Camera(width=FRAME_WIDTH, height=FRAME_HEIGHT, fps=20, fmt=PixelFormat.BGR) as cam:
        print(f"✅ MindBoard Virtual Camera started! Starting with {current_app} app.")
        
        while True:
            if keyboard.is_pressed('m'):
                mirror_output = not mirror_output  # Toggle mirroring
                print(f"Mirror mode {'enabled' if mirror_output else 'disabled'}.")
                time.sleep(0.3)  # Prevent multiple toggles

            if current_app == "eye_gaze":
                run_eye_gaze_keyboard(cap, cam)
                current_app = "air_painting" 
            elif current_app == "air_painting":
                run_air_painting(cap, cam)
                current_app = "eye_gaze"

            # A brief pause to prevent the 'd' key press from being immediately read by the next app
            time.sleep(0.5)

            if keyboard.is_pressed('q'):
                print("Exiting application.")
                break

    print("Releasing resources.")
    cap.release()
    cv2.destroyAllWindows()
